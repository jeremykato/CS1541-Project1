------FIVE STAGE COUNTS: TEAM zacalbertjeremy -----
--------------SAMPLE TRACE-------------------

 ** opening file /afs/cs.pitt.edu/courses/1541/short_traces/sample.tr
+ Simulation terminates at cycle : 1156

 ** opening file /afs/cs.pitt.edu/courses/1541/short_traces/sample.tr
+ Simulation terminates at cycle : 1150
--------------LONG TRACES-------------------

 ** opening file /afs/cs.pitt.edu/courses/1541/long_traces/sample_large1.tr
+ Simulation terminates at cycle : 112886972

 ** opening file /afs/cs.pitt.edu/courses/1541/long_traces/sample_large1.tr
+ Simulation terminates at cycle : 104751986

 ** opening file /afs/cs.pitt.edu/courses/1541/long_traces/sample_large2.tr
+ Simulation terminates at cycle : 125422888

 ** opening file /afs/cs.pitt.edu/courses/1541/long_traces/sample_large2.tr
+ Simulation terminates at cycle : 117509911
--------------SHORT TRACES-------------------

 ** opening file /afs/cs.pitt.edu/courses/1541/short_traces/sample1.tr
+ Simulation terminates at cycle : 1267766

 ** opening file /afs/cs.pitt.edu/courses/1541/short_traces/sample1.tr
+ Simulation terminates at cycle : 1129674

 ** opening file /afs/cs.pitt.edu/courses/1541/short_traces/sample2.tr
+ Simulation terminates at cycle : 1215048

 ** opening file /afs/cs.pitt.edu/courses/1541/short_traces/sample2.tr
+ Simulation terminates at cycle : 1224391

 ** opening file /afs/cs.pitt.edu/courses/1541/short_traces/sample3.tr
+ Simulation terminates at cycle : 1351474

 ** opening file /afs/cs.pitt.edu/courses/1541/short_traces/sample3.tr
+ Simulation terminates at cycle : 1293841

 ** opening file /afs/cs.pitt.edu/courses/1541/short_traces/sample4.tr
+ Simulation terminates at cycle : 3834281

 ** opening file /afs/cs.pitt.edu/courses/1541/short_traces/sample4.tr
+ Simulation terminates at cycle : 3647921


------SUPERSCALAR COUNTS: TEAM zacalbertjeremy -----
--------------SAMPLE TRACE-------------------

 ** opening file /afs/cs.pitt.edu/courses/1541/short_traces/sample.tr
+ Simulation terminates at cycle : 1042
--------------LONG TRACES-------------------

 ** opening file /afs/cs.pitt.edu/courses/1541/long_traces/sample_large1.tr
+ Simulation terminates at cycle : 98969051

 ** opening file /afs/cs.pitt.edu/courses/1541/long_traces/sample_large2.tr
+ Simulation terminates at cycle : 118631313
--------------SHORT TRACES-------------------

 ** opening file /afs/cs.pitt.edu/courses/1541/short_traces/sample1.tr
+ Simulation terminates at cycle : 1162972

 ** opening file /afs/cs.pitt.edu/courses/1541/short_traces/sample2.tr
+ Simulation terminates at cycle : 1111579

 ** opening file /afs/cs.pitt.edu/courses/1541/short_traces/sample3.tr
+ Simulation terminates at cycle : 1297600

 ** opening file /afs/cs.pitt.edu/courses/1541/short_traces/sample4.tr
+ Simulation terminates at cycle : 3572145


Implementation notes the 5-stage pipeline simulator

	The biggest new addition to the 5-stage pipeline is the branch prediction table. To implement this, we added a struct prediction type into CPU.h which encapsulates an instructions prediction and the predicted branch target address. We also created a simple macro called HASH() which extracts bits 4-9 from a 32-bit address to index the branch prediction table. HASH() performs a bit shift right by 4 and then a 6-bit-mask (bitwise AND with 0x3F). We initialize the table to predict NOT TAKEN for all branches which have not been encountered. As the pipeline proceeds, we first check for a prediction from the table for all jumps and branches. If we predict either the target or the branch incorrectly, we have to squash on the next cycle. We used a second simple function to update the BTB with the new prediction and target. This allows us to navigate the control hazards in the pipeline.

	For data hazards, we check for read after write dependencies. To be precise, the data hazard we must detect is a load word instruction followed by any instruction that reads from the register written in the load instruction. If the instruction before is not a load, we return 0 to show that there is not data hazard for that cycle. Otherwise, if either sReg_a or sReg_b was read from in the next instruction, where sReg_a or sReg_b equals to the dReg for the load instruction, then we return 1 to show that there is a data hazard. To fully resolve the data hazard, following the load instruction we insert a no-op to make sure there is a cycle to allow the register to be written before the next instruction retrieves the data.

	To validate our results, we constructed short trace files to test the way we handled branches and data dependencies. For example, in our control hazard trace we tested to see if our branch prediction properly updated the last branch taken in a for loop. In this case, initially the branch was predicted not taken, although the for loop iterated once. Finally, when the for loop ended, the branch predictor predicted the branch was taken, thus we mispredict a second time, and end the trace shortly after. For data hazards, in another trace file we tested alternating between R-Type instructions and Load Word instructions. Some R-Type instructions depended on a previous lw instruction, and in response a no-op was inserted wherever such a dependency occurred. We also had a few control load instructions with no dependencies to ensure we were not triggering false positives in the simulation.

Implementation notes the superscalar pipeline simulator

        To transition from the five-stage pipeline to the superscalar required first duplicating each pipeline stage and adding a PACKING buffer to hold the next super instruction to run. We then had the trace print the results from both writeback stages per cycle. Unlike the non-pipelined version, we needed more sophisticated logic to handle the fact that we might finish either 0, 1, or 2 instructions per cycle. To handle this, we kept track of how many instructions were packed each cycle in an integer counter which reset to 0 each cycle and adjusted the pacing of the pipeline accordingly.

        Next, we added logic which packed the instructions from the PREFETCH queue into the correct pipes. This was pretty tricky to get right, and we literally whiteboarded all the possible types of combinations and rubber-ducked them through the if-else conditionals which decided how to pack the instructions. It was also critical that we never ever allow a second instruction to be packed if the first one needed to stall. 

        Once we had the pipeline pulling two instructions per cycle down the correct pipes, the rest of the logic wast that difficult to implement. To check for load/use hazard, we could simply reuse the function from the 5-stage pipeline and just run the check on two instructions. The only real difference is that there were really two scenarios now -- one where the first instruction was dependent and we have a double-NOOP cycle and the other where only the second instruction was dependent but we could still pack the first instruction with one NOOP. This took care of the load-use hazard.

        Control hazard handling in the superscalar pipe was very simple. We pack branches and jumps which occur first lexically by themselves to avoid making mistakes, though putting a lw/sw instruction with the branch/jump is okay if the lw/sw was lexically first. Since we have no branch prediction, all we need to do is squash the whole next cycle any time a branch is taken or we execute a jump.

        Lastly, we added one new function to check for data dependencies between instructions being loaded in the same cycle. This is a lot like the load/use function from before except that the first instruction didnt necessarily need to be a load. It could potentially any instruction which writes a register that the other instruction needs to read. If such a hazard is detected, we simply pack the first instruction by itself and stall the second instruction until the next cycle.

	In addition to using the provided trace files to test the superscalar pipeline, we built a small sample trace (hazard_test_ss.tr) which checked for several data hazard scenarios which required the pipeline to stall--particularly where multiple hazards chained together to make sure that no instructions were being packed incorrectly or executed out-of-order. This was a tricky trace and revealed several bugs in the early drafts of our superscalar simulator.

Notes for both versions

	To make the pipeline calculate the correct cycle numbers, we needed to adjust the flush counter from 4 to 6 to account for the presence of the two stages prior to IF in both simulators. In the 5-stage pipe, this was PREFETCH[0] and PREFETCH[1], and in the superscalar version this was PREFETCH and PACKING. To adjust for the two shadow cycles that the pipeline executes prior to loading the first instruction into IF, we initialized cycle_counter to -2 at the beginning of the program. While probably not the most elegant solution possible, this offset corrected for the 2-cycle disparity in the end result of the simulator.


Analysis:


        CYCLES TAKEN TABLE:

        DATA:               Predict - 0     Predict - 1     Pred. 1 / Pred. 0   Superscalar     Super. / Pred. 0
        ========================================================================================================
        sample              1156            1150            -0.52%              1042            -9.86%
        sample1             1267766         1129674         -12.2%              1162972         -8.27%
        sample2             1215048         1224391         +0.76%              1111579         -8.52%
        sample3             1351474         1293841         -4.45%              1297600         -3.99%
        sample4             3834281         3647921         -5.11%              3572145         -6.84%
        sample_large1       112886972       104751986       -7.77%              98969051        -12.3%
        sample_large2       125422888       117509911       -6.73%              118631313       -5.41%

        *Positive Values indicate that the alternative versions took longer than Predict Not-Taken, negative values indicate that the alternative versions had performance increases.

        BRANCH PREDICTION'S REDUCTION IN SQUASHED INSTRUCTIONS:

        Instruction Count:  Branches    J Type      JR Type     No-Ops (w/Prediction)   No-Ops (w/e prediction) No-Op Reduction
        ========================================================================================================================
        sample              141         34          20          119                     125                     -4.80%
        sample1             170329      33836       820         32018                   170110                  -81.1%
        sample2             163431      33089       23538       139991                  130648                  +7.15%
        sample3             220846      62957       14691       101012                  158645                  -36.3%
        sample4             554606      129882      39049       273044                  459404                  -40.6%
        sample_large1       13103881    3723842     1140898     4968334                 13103320                -62.1%
        sample_large2       21107216    5286545     1211301     7651074                 15564051                -50.8%

        Branch Prediction Analysis

        Branch prediction was often able to massively reduce the number of incorrectly predicted branches in comparison to the Predict Not-Taken version of the same model. Though the overall improvement in the performance of the total number of cycles was often times small, branch prediction was almost always an improvement over defaulting to predicting Not-Taken.

        In each of the files, with the exception of sample and sample2, branch prediction was able to cut down on the number of cycles executed by a solid amount. The difference that branch prediction made is much more clear in the second table - in all traces except for sample and sample2, branch prediction made massive improvements in the number of no-op instructions that had to be inserted into the pipeline, with sample1 reducing over 80% of the total squashed instructions. Especially in the traces that contained a significantly higher ratio of branches to total instructions, branch prediction made a massive difference in the corresponding cycle performance as well. Sample1 as sample_large1's numerous branch instructions meant that branch prediction's impact was much larger when it could prevent opportunities for stalling that the original pipeline would otherwise encounter.

        In the case of sample2, where the performance degraded instead of improving, there are two scenarios that could lead to worse performance than predicting not-taken, the second being the most likely:
        
                1) The branch prediction for a given branch repeatedly got given branches wrong - however, unless the branch alternated between taken & taken and not taken & not taken, this is unlikely.
                2) When hashing the address of the branch, collisions occurred, causing the hash table to frequently predict results incorrectly, which is especially likely if a collision exists, and the colliding addresses's typical result is the opposite of one another.
        
        It's possible that sample2 had one or more instances where branch values collided with one another and repeatedly overwrote one another's prediction, causing the table to consistently predict the wrong value. This could likely be prevented with either a larger hash table that would reduce the possibility of this occurring.


        Superscalar Performance Analysis

        Note: The Superscalar performance is compared to the Predict Not-Taken performance since they use the same means of resolving branches.

        The Superscalar version of the processor was able to increase performance by a somewhat significant margin in all of the trace files, but despite having two pipelines, the maximum performance improvement of the pipeline was only 12.3%. Though in theory the two pipelines could theoretically double performance, in reality, gains are nowhere near that large due to a variety of factors: 

                1) Branches can only be paired with a lw/sw instruction that precedes it.
                2) Consecutive instructions frequently have data dependencies on one another. Operations such as loading a value from memory, performing arithmetic on it, and then storing the value to memory are likely common scenarios in reality.
                3) The pipeline must stall one "pipe" of the processor if it's either given consecutive ALU/Branch/Special instructions or consecutive lw/sw instructions. Whenever this happens, there is no way for the processor to take advantage of the unused pipeline.

        What is notably important is that items 2) and 3) could likely be solved with a compiler capable of producing optimizing instructions, swapping their order to take advantage of the multiple pipelines, or means for the processor to engage in Out-Of-Order-Execution, where it would be able to situations where there were no data dependencies on instructions in order to fill unused pipes.

        Nevertheless, the Superscalar was always faster than the single pipeline version and produced equivalent results to that of the single pipeline's execution of the same code. Since both the single-pipeline and the Superscalar deal with branches and data dependencies in similar manners, we can reason that the performance increase in each sample trace is the number of instructions where the processor was able to execute 2 instructions at once, thus reducing the cycle count.

        For instance, in sample1_large.tr, the total number of cycles were reduced by 12.3%, which must mean that 24.6% of the instructions were cases that two consecutive instructions (that didn't have dependencies, branches preceding another instruction, etc) could be run in parallel. Though many of the other performance increases were only marginal, it is more clear why it is so difficult to parallelize more instructions. Even in the most-optimal case of the examples, nearly a quarter of instructions had to fit in the "ideal" scenario in order to be run in parallel.

